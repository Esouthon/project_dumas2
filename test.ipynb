{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger le tokenizer et le modèle BERT pré-entraîné\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Exemple de phrase\n",
    "k=0\n",
    "L=[]\n",
    "file1='/Users/eliotsouthon/Desktop/Projet Dumas/512_sans_lignes_vides.csv'\n",
    "df = pd.read_csv(file1)\n",
    "text=[df['Block'][k]]\n",
    "\n",
    "for k in\n",
    "# Tokenizer la phrase\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Récupérer les embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Récupérer les vecteurs de sortie\n",
    "# `outputs.last_hidden_state` contient les embeddings pour chaque token\n",
    "# `outputs.pooler_output` contient le vecteur pour le token [CLS]\n",
    "token_embeddings = outputs.last_hidden_state\n",
    "cls_embedding = outputs.pooler_output\n",
    "\n",
    "# Afficher les dimensions des sorties\n",
    "print(\"Taille de chaque token embedding :\", token_embeddings.size())\n",
    "print(\"Taille du CLS embedding :\", cls_embedding.size())\n",
    "print(cls_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1='/Users/eliotsouthon/Desktop/Projet Dumas/fichier_combined.csv'\n",
    "df = pd.read_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Block'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df['Block'][0]\n",
    "# Tokenizer la phrase\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Récupérer les embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Récupérer les vecteurs de sortie\n",
    "# `outputs.last_hidden_state` contient les embeddings pour chaque token\n",
    "# `outputs.pooler_output` contient le vecteur pour le token [CLS]\n",
    "token_embeddings = outputs.last_hidden_state\n",
    "cls_embedding = outputs.pooler_output\n",
    "\n",
    "# Afficher les dimensions des sorties\n",
    "print(\"Taille de chaque token embedding :\", token_embeddings.size())\n",
    "print(\"Taille du CLS embedding :\", cls_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 517/517 [00:04<00:00, 113.56 examples/s]\n",
      "Map: 100%|██████████| 130/130 [00:01<00:00, 117.44 examples/s]\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "65/65 [==============================] - 373s 6s/step - loss: 0.8598 - accuracy: 0.4952 - val_loss: 0.6931 - val_accuracy: 0.4385\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 376s 6s/step - loss: 0.6931 - accuracy: 0.4313 - val_loss: 0.6931 - val_accuracy: 0.4385\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 370s 6s/step - loss: 0.6931 - accuracy: 0.4275 - val_loss: 0.6931 - val_accuracy: 0.4385\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 366s 6s/step - loss: 0.6931 - accuracy: 0.4352 - val_loss: 0.6931 - val_accuracy: 0.4385\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 373s 6s/step - loss: 0.6931 - accuracy: 0.4352 - val_loss: 0.6931 - val_accuracy: 0.4385\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Layer tf_bert_for_sequence_classification_2 has no inbound nodes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39minput, outputs\u001b[38;5;241m=\u001b[39msoftmax_output)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Créer le modèle avec la couche Softmax\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m model_with_softmax \u001b[38;5;241m=\u001b[39m \u001b[43madd_softmax_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Exemple d'inférence\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_probabilities\u001b[39m(texts):\n",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m, in \u001b[0;36madd_softmax_layer\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_softmax_layer\u001b[39m(model):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Obtenir les logits du modèle BERT\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Ajouter une couche Softmax\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     softmax_output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mSoftmax()(logits)\n",
      "File \u001b[0;32m~/Desktop/Projet Dumas/.venv/lib/python3.9/site-packages/tf_keras/src/engine/base_layer.py:2082\u001b[0m, in \u001b[0;36mLayer.output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the output tensor(s) of a layer.\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \n\u001b[1;32m   2070\u001b[0m \u001b[38;5;124;03mOnly applicable if the layer has exactly one output,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;124;03m  RuntimeError: if called in Eager mode.\u001b[39;00m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes:\n\u001b[0;32m-> 2082\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   2083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m has no inbound nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2084\u001b[0m     )\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_node_attribute_at_index(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Layer tf_bert_for_sequence_classification_2 has no inbound nodes."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFBertModel\n",
    "from datasets import Dataset\n",
    "\n",
    "# Lire le fichier CSV\n",
    "file1 = '/Users/eliotsouthon/Desktop/Projet Dumas/512_sans_lignes_vides.csv'\n",
    "df = pd.read_csv(file1)\n",
    "\n",
    "# Diviser les données\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Charger le tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Fonction pour tokeniser les données\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Block'], padding='max_length', truncation=True, max_length=128, return_tensors='tf')\n",
    "\n",
    "# Convertir les DataFrames en datasets Hugging Face\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokeniser les datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Préparer les datasets TensorFlow\n",
    "def convert_to_tf_dataset(dataset):\n",
    "    features = {x: dataset[x] for x in tokenizer.model_input_names}\n",
    "    labels = dataset['label']\n",
    "    return tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)\n",
    "\n",
    "train_tf_dataset = convert_to_tf_dataset(train_dataset)\n",
    "test_tf_dataset = convert_to_tf_dataset(test_dataset)\n",
    "\n",
    "# Charger le modèle pré-entraîné BERT pour la classification\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Utiliser l'optimiseur Keras hérité\n",
    "legacy_optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=2e-5)\n",
    "\n",
    "# Configurer l'entraînement\n",
    "model.compile(optimizer=legacy_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraîner le modèle et capturer l'historique\n",
    "history = model.fit(train_tf_dataset, epochs=5, validation_data=test_tf_dataset)\n",
    "\n",
    "# Ajouter une couche Softmax pour obtenir des probabilités\n",
    "def add_softmax_layer(model):\n",
    "    # Obtenir les logits du modèle BERT\n",
    "    logits = model.output\n",
    "    # Ajouter une couche Softmax\n",
    "    softmax_output = tf.keras.layers.Softmax()(logits)\n",
    "    # Créer un nouveau modèle avec la couche Softmax\n",
    "    return tf.keras.Model(inputs=model.input, outputs=softmax_output)\n",
    "\n",
    "# Créer le modèle avec la couche Softmax\n",
    "model_with_softmax = add_softmax_layer(model)\n",
    "\n",
    "# Exemple d'inférence\n",
    "def predict_probabilities(texts):\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
    "    logits = model_with_softmax(encodings['input_ids'], attention_mask=encodings['attention_mask'])\n",
    "    return logits.numpy()\n",
    "\n",
    "# Tester avec un exemple\n",
    "sample_texts = [\"Alors, au fond de ce cœur malade naquit le premier germe d’un ulcère mortel. Cet homme qu’il sacrifiait à son ambition, cet innocent qui ôtait pour son père coupable, lui apparut pâle et menaçant, donnat la main à sa fiancée, pâle comme lui, et trainant après lui le remords non pas celui qui fait bondir le malade comme les furieux de la fatalité antique, mais ce tintement sourd et douloureux qui, à certains moments,frappe sur le cœur et le meurtrit au souvenir d’une action passée, meurtrissure dont les lancinants douleurs creusent un mal qui  va s’approfondissant jusqu’à la mort. \", \"Il ne chercha pas à essuyer les larmes qui roulaient sur les joues de Mercedes ; et cependant, pour chacune de ces larmes, il eut donné un verre de son sang ; mais ces larmes coulaient pour un autre.\"]\n",
    "probabilities = predict_probabilities(sample_texts)\n",
    "\n",
    "print(\"Probabilités pour chaque exemple:\")\n",
    "for text, prob in zip(sample_texts, probabilities):\n",
    "    print(f\"Texte : {text}\")\n",
    "    print(f\"Probabilités : {prob}\")\n",
    "\n",
    "# Tracer les courbes de perte et d'exactitude\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Courbe de perte\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Loss (Train)')\n",
    "plt.plot(history.history['val_loss'], label='Loss (Validation)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Courbe d'exactitude\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy (Train)')\n",
    "plt.plot(history.history['val_accuracy'], label='Accuracy (Validation)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "# Lire le fichier CSV\n",
    "file1 = '/Users/eliotsouthon/Desktop/Projet Dumas/512_sans_lignes_vides.csv'\n",
    "df = pd.read_csv(file1)\n",
    "\n",
    "# Diviser les données\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Charger le tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Fonction pour tokeniser les données\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Block'], padding='max_length', truncation=True, max_length=512, return_tensors='tf')\n",
    "\n",
    "# Convertir les DataFrames en datasets Hugging Face\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokeniser les datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Préparer les datasets TensorFlow\n",
    "def convert_to_tf_dataset(dataset):\n",
    "    features = {x: dataset[x] for x in tokenizer.model_input_names}\n",
    "    labels = dataset['label']\n",
    "    return tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)\n",
    "\n",
    "train_tf_dataset = convert_to_tf_dataset(train_dataset)\n",
    "test_tf_dataset = convert_to_tf_dataset(test_dataset)\n",
    "\n",
    "# Charger le modèle pré-entraîné BERT pour la classification\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Configurer l'entraînement\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(train_tf_dataset, epochs=5, validation_data=test_tf_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
